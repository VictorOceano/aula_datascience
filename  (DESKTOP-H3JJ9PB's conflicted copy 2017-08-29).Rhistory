View(Atl_race_transform)
mapview(left_join(Atl_race,Atl_race_transform))
mapview(left_join(Atl_race,Atl_race_transform),by="GEOID")
st_geometry(Atl_race_transform)<-NULL
library(sf)
Atl_race_transform<-Atl_race %>%
mutate(pct = 100 * (value / summary_value),
variable = fct_recode(variable,
WhtAm = "P0050003",
AfAm = "P0050004",
AsnAm = "P0050006",
Latinx = "P0040003")) %>%
select(-value) %>%
filter(summary_value!=0) %>%
spread(variable,pct)
tm_shape(Atl_race_transform) +
tm_polygons(c("WhtAm","AfAm","AsnAm","Latinx"),
style="jenks",
palette=list("Reds", "Purples","Blues", "YlGn"),
auto.palette.mapping=FALSE,
title=c("% White American", "% African American","% Asian American","% Latinx"))+
tm_scale_bar() +
tm_compass()
library(tidyverse)
library(readr)
census_data <- read_csv("C:/Users/jshannon/Dropbox/Jschool/Teaching/Courses/Geog4300_6300 Fall 17/geog4300/Data/ACSCtyData_2014ACS.csv")
View(ACSCtyData_2014ACS)
View(census_data)
census_data<-read_csv("https://github.com/jshannon75/geog4300/raw/master/Data/ACSCtyData_2014ACS.csv")
str(census_data)
head(census_data)
tail(census_data)
hist(census_data$pov_pop_pct)
boxplot(census_data$pov_pop_pct)
census_data_se<-census_data %>%
filter(Region=="SE")
View(census_data_se)
table(census_data$St_name)
table(census_data$St_name,census_data$Region)
census_data_pov<-census_data %>%
select(pov_pop_pct>30)
census_data_pov<-census_data %>%
dplyr::select(pov_pop_pct>30)
?select
census_data_pov<-census_data %>%
select(pov_pop_pct>30)
census_data_se<-census_data %>%
filter(Region=="SE")
View(census_data_se)
census_data_pov<-census_data %>%
filter(pov_pop_pct>30)
View(census_data_pov)
table(census_data_pov$St_name)
census_data_sepov<-census_data %>%
filter(Region=="SE") %>%
filter(pov_pop_pct>30)
View(census_data_pov)
View(census_data_sepov)
names(census_data)
names(census_data)
census_data_pov<-census_data %>%
select(GEOID,St_name,Region,pov_pop_pct)
census_data_pov<-census_data %>%
select(1,5,6,47)
survey<-read_csv("https://github.com/jshannon75/geog4300/raw/master/Data/geog4300_survey.csv")
D<-c(1,"Map",2,"Globe",3,"Atlas")
D
census_data$less_BA_pct<-census_data$LessHS_pct+census_data$HSGrad_pct+census_data$SomeCol_pct
hist(census_data$less_BA_pct)
hist(census_data$)
hist(census_data$BADeg_pct)
mean(A)
A<-c(1,2,3,4,5)
mean(A)
median(A)
sd(A)
summary(A)
sum(A)
sum_A<-sum(A)
sum_A
B<-(3,5,3,2,6)
C<-data_frame(A,B)
C<-data.frame(A,B)
B<-(3,5,3,2,6)
A<-(1,2,3,4,5)
B<-(3,5,3,2,6)
C<-data.frame(A,B)
B<-(3,5,3,2,6)
A<-c(1,2,3,4,5)
B<-c(3,5,3,2,6)
C<-data.frame(A,B)
C
C<-t(C)
C
library(httr)
library(jsonlite)
library(magrittr)
alderaan <- GET("http://swapi.co/api/planets/?search=alderaan")
alderaan
summary(alderaan)
alderaan$url
alderaan$status_code
alderaan$headers$`content-type`
names(alderaan)
text_content <- content(alderaan, as = "text", encoding = "UTF-8")
text_content
parsed_content <- content(alderaan, as = "parsed")
names(parsed_content)
parsed_content$count
str(parsed_content$results)
parsed_content$results[[1]]$name
parsed_content$results[[1]]$terrain
json_content <- text_content %>% fromJSON
json_content
planetary_data <- json_content$results
names(planetary_data)
planetary_data$name
planetary_data$terrain
View(planetary_data)
json_parse <- function(req) {
text <- content(req, as = "text", encoding = "UTF-8")
if (identical(text, "")) warn("No output to parse.")
fromJSON(text)
}
parsed_next_page <- json_parse(next_page)
next_page <- GET(json_planets$`next`) %>% stop_for_status()
parsed_next_page <- json_parse(next_page)
planets <- GET("http://swapi.co/api/planets") %>%
stop_for_status() %>%
json_parse
swapi_planets <- planets$results
View(swapi_planets)
next_page <- planets$`next`
while(!is.null(next_page)) {
more_planets <- GET(next_page) %>%
stop_for_status() %>%
json_parse
swapi_planets <- rbind(swapi_planets, more_planets$results)
next_page <- more_planets$`next`
}
length(swapi_planets$name)
swapi_planets$name
View(swapi_planets)
library(httr)
library(jsonlite)
library(magrittr)
dagobah <- GET("http://swapi.co/api/planets/?search=dagobah")
dagobah <- GET("http://swapi.co/api/planets/?search=dagobah")
dagobar$status_code
dagobah$status_code
dagobah$headers$`content-type`
dagobah$headers$`content-type`
names(alderaan)
names(dagobah) #see what's in this object
text_content <- content(dagobah, as = "text", encoding = "UTF-8")
text_content
json_content <- text_content %>% fromJSON
json_content
planetary_data <- json_content$results
names(planetary_data)
planetary_data$name
planetary_data$terrain
View(planetary_data)
json_content
planetary_data <- json_content$results
names(planetary_data)
planetary_data$name
planetary_data$terrain
planets <- GET("http://swapi.co/api/planets") %>% stop_for_status()
json_parse <- function(req) {
text <- content(req, as = "text", encoding = "UTF-8")
if (identical(text, "")) warn("No output to parse.")
fromJSON(text)
}
planets <- GET("http://swapi.co/api/planets") %>% stop_for_status()
json_planets <- json_parse(planets)
swapi_planets <- json_planets$results
View(swapi_planets)
library(daymetr) # load the package
install.packages("daymetr")
install.packages("daymetr")
download.daymet(site="test",lat=39.598,lon=-93.518,start_yr=2005,end_yr=2015,internal=TRUE)
devtools::install_github("khufkens/daymetr")
download.daymet(site="test",lat=39.598,lon=-93.518,start_yr=2005,end_yr=2015,internal=TRUE)
library(daymetr) # load the package
download.daymet(site="test",lat=39.598,lon=-93.518,start_yr=2005,end_yr=2015,internal=TRUE)
download_daymet(site="test",lat=39.598,lon=-93.518,start_yr=2005,end_yr=2015,internal=TRUE)
download_daymet(site="test",lat=39.598,lon=-93.518,start=2005,end=2015,internal=TRUE)
test<-download_daymet(site="test",lat=39.598,lon=-93.518,start=2005,end=2015,internal=TRUE)
download_daymet(site="athens",lat=33.948693,lon=-83.375475,start=2005,end=2015,internal=TRUE)
athens_2005_2015 <- read_csv("athens_2005_2015.csv", skip = 7) #The skip command here skips the first few rows which have metadata.
library(tidyverse)
athens_2005_2015 <- read_csv("athens_2005_2015.csv", skip = 7) #The skip command here skips the first few rows which have metadata.
View(athens_2005_2015)
library(tidyverse)
library(jsonlite)
library(magrittr)
dagobah <- GET("http://swapi.co/api/planets/?search=dagobah") #retrieve content
dagobah$headers$`content-type` #look at the format
dagobah <- GET("http://swapi.co/api/planets/?search=dagobah") #retrieve content
library(httr)
dagobah <- GET("http://swapi.co/api/planets/?search=dagobah") #retrieve content
dagobah$headers$`content-type` #look at the format
names(dagobah) #see what's in this object. We'll eventually want just "content".
text_content <- content(dagobah, as = "text", encoding = "UTF-8")
text_content #The content function gets us closer, but still not there.
json_content <- text_content %>% fromJSON
json_content
planetary_data <- json_content$results
names(planetary_data)
planets <- GET("http://swapi.co/api/planets") %>% stop_for_status()
json_parse <- function(req) {
text <- content(req, as = "text", encoding = "UTF-8")
if (identical(text, "")) warn("No output to parse.")
fromJSON(text)
}
planets <- GET("http://swapi.co/api/planets") %>% stop_for_status()
json_planets <- json_parse(planets)
swapi_planets <- json_planets$results
library(daymetr) # load the package
download_daymet(site="athens",lat=33.948693,lon=-83.375475,start=2005,end=2015,internal=TRUE)
athens_2005_2015 <- read_csv("athens_2005_2015.csv", skip = 7) #The skip command here skips the first few rows which have metadata.
athens_daymet<-athens_2005_2015 %>%
rename(dayl=`dayl (s)`,
tmax=`tmax (deg c)`) %>%
select(year,yday,dayl,tmax)
View(athens_daymet)
athens_daymet<-athens_2005_2015 %>%
mutate(dayl_hr=dayl/360)
athens_daymet<-athens_2005_2015 %>%
mutate(dayl_hr= dayl / 360)
athens_daymet<-athens_2005_2015 %>%
mutate(dayl_hr= dayl / 360)
athens_daymet<-athens_2005_2015 %>%
mutate(dayl_hr = dayl / 360)
?mutate
athens_daymet<-athens_2005_2015 %>%
mutate(., dayl_hr = dayl / 360)
athens_daymet<-athens_daymet %>%
mutate(dayl_hr = dayl / 360)
athens_daymet<-athens_daymet %>%
mutate(dayl_hr = dayl / 3600) #60*60=3600 seconds per hour
athens_daymet_summary <- athens_daymet %>%
group_by(year) %>%
summarise(tmax_mean=mean(tmax))
View(athens_daymet_summary)
day_month<-read_csv("https://raw.githubusercontent.com/jshannon75/geog4300/master/Data/daymet%20day_month.csv")
athens_daymet_summary<-athens_daymet_summary %>%
left_join(day_month)
athens_dayment_month<-left_join(athens_daymet,day_month)
athens_daymet_month_summary<-athens_daymet %>%
group_by(year,month) %>%
summarise(tmax_mean=mean(tmax))
athens_daymet_month_summary<-athens_daymet_month %>%
group_by(year,month) %>%
summarise(tmax_mean=mean(tmax))
athens_daymet_month<-left_join(athens_daymet,day_month)
athens_daymet_month_summary<-athens_daymet_month %>%
group_by(year,month) %>%
summarise(tmax_mean=mean(tmax))
View(athens_daymet_month_summary)
athens_daymet_month_summary<-athens_daymet %>%
left_join(day_month) %>%
group_by(year,month) %>%
summarise(tmax_mean=mean(tmax))
View(athens_daymet_month_summary)
athens_daymet_month<-left_join(athens_daymet,day_month)
athens_daymet_month_summary<-athens_daymet_month %>%
group_by(year,month) %>%
summarise(tmax_mean=mean(tmax))
athens_daymet_month_summary<-athens_daymet %>%
left_join(day_month) %>%
group_by(year,month) %>%
summarise(tmax_mean=mean(tmax))
censusdata<-st_read("https://github.com/jshannon75/geog4300/raw/master/Data/ACSCtyData_2014ACS.shp")
library(tidyverse)
library(sf)
library(tmap)
censusdata<-st_read("https://github.com/jshannon75/geog4300/raw/master/Data/ACSCtyData_2014ACS.shp")
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp")
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp")
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp")
library(tidyverse)
library(sf)
library(tmap)
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp")
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp")
censusdata<-st_read("https://github.com/jshannon75/geog4300/raw/master/Data/ACSCtyData_2014ACS.shp")
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp")
summary(censusdata$FB_INS_PCT)
jenks<-c(10,35,52,72)
tm_shape(censusdata) +
tmborders("grey20")
tm_shape(censusdata) +
tm_borders("grey20")
View(censusdata)
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp") %>%
mutate(state=substr(GEOID,1,2))
filter(state!="15")
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp") %>%
mutate(state=substr(GEOID,1,2)) %<%
filter(state!="15")
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp") %>%
mutate(state=substr(GEOID,1,2)) %<%
filter(state!="15")
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp") %>%
mutate(state=substr(GEOID,1,2)) %>%
filter(state!="15")
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp") %>%
mutate(state=substr(GEOID,1,2)) %>%
filter(state!="15" * state !="02" )
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp") %>%
mutate(state=substr(GEOID,1,2)) %>%
filter(state!="15" & state !="02" )
tm_shape(censusdata) +
tm_borders("grey20")
tm_shape(censusdata) +
tm_polygons(c("NAT_INS_PC","NAT_INS_PC"))
?tm_polygons
jenksmap<-tm_shape(censusdata) +
tm_polygons("NAT_INS_PC",breaks=jenks)
jenksmap
jenksmap<-tm_shape(censusdata) +
tm_polygons("FB_INS_PC",breaks=jenks)
jenksmap
jenksmap<-tm_shape(censusdata) +
tm_polygons("FB_INS_PCT",breaks=jenks)
summary(censusdata$NB_INS_PCT)
summary(censusdata$NAT_INS_PC)
jenks<-c(50,83,86,90)
jenksmap<-tm_shape(censusdata) +
tm_polygons("NAT_INS_PC",breaks=jenks)
jenksmap
jenks<-c(0,50,83,86,90)
jenksmap<-tm_shape(censusdata) +
tm_polygons("NAT_INS_PC",breaks=jenks)
jenksmap
jenks<-c(0,50,83,86,90)
quantile<-c(0,50,83,86,90)
jenksmap<-tm_shape(censusdata) +
tm_polygons("NAT_INS_PC",breaks=jenks)+
tm_polygons("NB_INS_PC",breaks=quantile)
jenksmap
jenksmap<-tm_shape(censusdata) +
tm_polygons("NAT_INS_PC",breaks=jenks)+
tm_shape(censusdata)+
tm_polygons("NB_INS_PC",breaks=quantile)
jenksmap
jenksmap<-tm_shape(censusdata) +
tm_polygons("NAT_INS_PC",breaks=jenks)+
tm_shape(censusdata)+
tm_polygons("NAT_INS_PC",breaks=quantile)
jenksmap
tm_shape(censusdata) +
tm_polygons("NAT_INS_PC",breaks=jenks)+
tm_shape(censusdata)+
tm_polygons("NAT_INS_PC",breaks=quantile)+
tm_facets()
tm_shape(censusdata) +
tm_polygons(c("NAT_INS_PC","NAT_INS_PC")breaks=(jenks,quantile))+
tm_facets()
tm_shape(censusdata) +
tm_polygons(c("NAT_INS_PC","NAT_INS_PC")breaks=(jenks,quantile))+
tm_facets()
tm_shape(censusdata) +
tm_polygons(c("NAT_INS_PC","NAT_INS_PC"),breaks=(jenks,quantile))+
tm_facets()
tm_shape(censusdata) +
tm_polygons(c("NAT_INS_PC","NAT_INS_PC"),breaks=(jenks,quantile))+
tm_facets()
tm_shape(censusdata) +
tm_polygons(c("NAT_INS_PC","NAT_INS_PC"),breaks=c(jenks,quantile))+
tm_facets()
tm_shape(censusdata)+
tm_polygons("NAT_INS_PC",breaks=quantile)+
tm_facets()
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp") %>%
mutate(state=substr(GEOID,1,2),
natins1=NAT_INS_PC,
natins2=NAT_INS_PC) %>%
filter(state!="15" & state !="02" ) %>%
jenks<-c(0,50,83,86,90)
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp") %>%
mutate(state=substr(GEOID,1,2),
natins1=NAT_INS_PC,
natins2=NAT_INS_PC) %>%
filter(state!="15" & state !="02" ) %>%
jenks<-c(0,50,83,86,90)
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp") %>%
mutate(state=substr(GEOID,1,2),
natins1 = NAT_INS_PC,
natins2 = NAT_INS_PC) %>%
filter(state!="15" & state !="02" ) %>%
jenks<-c(0,50,83,86,90)
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp") %>%
mutate(state=substr(GEOID,1,2),
natins1 = NAT_INS_PC,
natins2 = NAT_INS_PC) %>%
filter(state!="15" & state !="02" )
jenks<-c(0,50,83,86,90)
quantile<-c(0,50,83,86,90)
tm_shape(censusdata) +
tm_polygons(c("natins1","natins2"),breaks=c(jenks,quantile))+
tm_facets()
jenks<-c(50,83,86,90)
quantile<-c(50,83,86,90)
tm_shape(censusdata) +
tm_polygons(c("natins1","natins2"),breaks=c(jenks,quantile))+
tm_facets()
tm_shape(censusdata) +
tm_polygons(c("natins1","natins2"),breaks=list(jenks,quantile))+
tm_facets()
jenks<-c(0,50,83,86,90,100)
quantile<-c(0,50,83,86,90,100)
jenks<-c(0,50,83,86,90,100)
quantile<-c(0,60,73,86,95,100)
natural_breaks<-c(0,30,60,80,90,100)
st_dev<-c(0,55,65,75,85,100)
tm_shape(censusdata) +
tm_polygons(c("natins_je","natins_qu","natins_nb","natins_sd"),
breaks=list(jenks,quantile))+
tm_facets()
tm_shape(censusdata) +
tm_polygons(c("natins_je","natins_qu","natins_nb","natins_sd"),
breaks=list(jenks,quantile,natural_breaks,st_dev))+
tm_facets()
censusdata<-st_read("Data/ACSCtyData_2014ACS.shp") %>%
mutate(state=substr(GEOID,1,2),
natins_je = NAT_INS_PC,
natins_qu = NAT_INS_PC,
natins_nb = NAT_INS_PC,
natins_sd = NAT_INS_PC) %>%
filter(state!="15" & state !="02" )
jenks<-c(0,50,83,86,90,100)
quantile<-c(0,60,73,86,95,100)
natural_breaks<-c(0,30,60,80,90,100)
st_dev<-c(0,55,65,75,85,100)
tm_shape(censusdata) +
tm_polygons(c("natins_je","natins_qu","natins_nb","natins_sd"),
breaks=list(jenks,quantile,natural_breaks,st_dev))+
tm_facets()
censusdata_csv<-read_csv("https://github.com/jshannon75/geog4300/raw/master/Data/ACSCtyData_2014ACS.csv")
hist(censusdata_csv)
hist(censusdata_csv$nat_ins_pct)
?hist
hist(censusdata_csv$nat_ins_pct,bins=30)
hist(censusdata_csv$nat_ins_pct,breaks=30)
summary(censusdata_csv$nat_ins_pct)
sd(censudata_csv$nat_ins_pct)
sd(censusdata_csv$nat_ins_pct)
tm_shape(censusdata) +
tm_polygons(c("natins_je","natins_qu","natins_nb","natins_sd"),
breaks=list(jenks,quantile,natural_breaks,st_dev))+
tm_facets()
library(httr)
library(tidyverse)
library(jsonlite)
library(magrittr)
dagobah <- GET("http://swapi.co/api/planets/?search=dagobah") #retrieve content
dagobah$headers$`content-type` #look at the format
names(dagobah) #see what's in this object. We'll eventually want just "content".
text_content <- content(dagobah, as = "text", encoding = "UTF-8")
?content
text_content <- content(dagobah, as = "text", encoding = "UTF-8") #command from httr package that loads the json content
text_content #The content function gets us closer, but still not there.
json_content <- text_content %>% fromJSON
json_content <- fromJSON(text_content)
json_content
planetary_data <- json_content$results
names(planetary_data)
planets <- GET("http://swapi.co/api/planets") %>% stop_for_status()
text <- content(req, as = "text", encoding = "UTF-8")
if (identical(text, "")) warn("No output to parse.")
fromJSON(text)
planets <- GET("http://swapi.co/api/planets") %>% stop_for_status()
json_planets <- json_parse(planets)
json_parse <- function(req) {
text <- content(req, as = "text", encoding = "UTF-8") #Parses the json file (same as l. 32)
if (identical(text, "")) warn("No output to parse.") #Checks that there's data
fromJSON(text) #Creates a list with data as output
}
planets <- GET("http://swapi.co/api/planets") %>% stop_for_status()
json_planets <- json_parse(planets)
swapi_planets <- json_planets$results
library(daymetr) # load the package
download_daymet(site="athens",lat=33.948693,lon=-83.375475,start=2005,end=2015,internal=TRUE)
athens_2005_2015 <- read_csv("athens_2005_2015.csv", skip = 7) #The skip command here skips the first few rows which have metadata.
athens_daymet<-athens_2005_2015 %>%
rename(dayl=`dayl (s)`,
tmax=`tmax (deg c)`) %>%
select(year,yday,dayl,tmax)
athens_daymet<-athens_daymet %>%
mutate(dayl_hr = dayl / 3600) #60*60=3600 seconds per hour
athens_daymet_summary <- athens_daymet %>%
group_by(year) %>%
summarise(tmax_mean=mean(tmax))
day_month<-read_csv("https://raw.githubusercontent.com/jshannon75/geog4300/master/Data/daymet%20day_month.csv")
athens_daymet_month<-left_join(athens_daymet,day_month)
athens_daymet_month_summary<-athens_daymet_month %>%
group_by(year,month) %>%
summarise(tmax_mean=mean(tmax))
athens_daymet_month_summary<-athens_daymet %>%
left_join(day_month) %>%
group_by(year,month) %>%
summarise(tmax_mean=mean(tmax))
View(athens_daymet_month_summary)
library(readr)
daymet_day_month <- read_csv("C:/Users/jshannon/Dropbox/Jschool/Teaching/Courses/Geog4300_6300 Fall 17/geog4300/Data/daymet day_month.csv")
View(daymet_day_month)
daymet_day_month<-unique(daymet_day_month)
write_csv(daymet_day_month,"C:/Users/jshannon/Dropbox/Jschool/Teaching/Courses/Geog4300_6300 Fall 17/geog4300/Data/daymet day_month.csv")
day_month<-read_csv("https://raw.githubusercontent.com/jshannon75/geog4300/master/Data/daymet%20day_month.csv")
